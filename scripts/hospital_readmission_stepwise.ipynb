{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import unicode_literals  \n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as linear_model\n",
    "\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/verena/deployment/interpretable-audio-models/iml_methods/influence-release/\")\n",
    "from influence.binaryLogisticRegressionWithLBFGS import BinaryLogisticRegressionWithLBFGS\n",
    "from influence.smooth_hinge import SmoothHinge\n",
    "import influence.dataset as dataset\n",
    "from influence.dataset import DataSet\n",
    "#from binaryLogisticRegressionWithLBFGS import BinaryLogisticRegressionWithLBFGS\n",
    "#from smooth_hinge import SmoothHinge\n",
    "#import dataset as dataset\n",
    "#from dataset import DataSet\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_vec(x, verbose=False):\n",
    "    assert len(feature_names) == len(x)\n",
    "    print('Age: %s' % x[age_var_indices])\n",
    "    if verbose:\n",
    "        for feature_name, val in zip(feature_names, x):\n",
    "            print('%32s: %.6f' % (feature_name, val))\n",
    "    \n",
    "def examine_train_point(idx, verbose=False):\n",
    "    print('Label: %s' % Y_train[idx])\n",
    "    examine_vec(modified_X_train[idx, :], verbose)\n",
    "    \n",
    "def examine_test_point(idx, verbose=False):\n",
    "    print('Label: %s' % Y_test[idx])\n",
    "    examine_vec(X_test[idx, :], verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/verena/experiments/influence/diabetic_data.csv') # TODO: dynamic path\n",
    "# Use this if you are not running this in CodaLab\n",
    "# df = pd.read_csv('../data/diabetic_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables into numeric ones\n",
    "\n",
    "X = pd.DataFrame()\n",
    "\n",
    "# Numerical variables that we can pull directly\n",
    "X = df.loc[:,  \n",
    "    [\n",
    "        'time_in_hospital',\n",
    "        'num_lab_procedures',\n",
    "        'num_procedures',\n",
    "        'num_medications',\n",
    "        'number_outpatient',\n",
    "        'number_emergency',\n",
    "        'number_inpatient',\n",
    "        'number_diagnoses'\n",
    "    ]]\n",
    "\n",
    "categorical_var_names = [\n",
    "    'gender',\n",
    "    'race',\n",
    "    'age', \n",
    "    'discharge_disposition_id',\n",
    "    'max_glu_serum',\n",
    "    'A1Cresult',\n",
    "    'metformin',\n",
    "    'repaglinide',\n",
    "    'nateglinide',\n",
    "    'chlorpropamide',\n",
    "    'glimepiride',\n",
    "    'acetohexamide',\n",
    "    'glipizide',\n",
    "    'glyburide',\n",
    "    'tolbutamide',\n",
    "    'pioglitazone',\n",
    "    'rosiglitazone',\n",
    "    'acarbose',\n",
    "    'miglitol',\n",
    "    'troglitazone',\n",
    "    'tolazamide',\n",
    "    'examide',\n",
    "    'citoglipton',\n",
    "    'insulin',\n",
    "    'glyburide-metformin',\n",
    "    'glipizide-metformin',\n",
    "    'glimepiride-pioglitazone',\n",
    "    'metformin-rosiglitazone',\n",
    "    'metformin-pioglitazone',\n",
    "    'change',\n",
    "    'diabetesMed'\n",
    "]\n",
    "\n",
    "for categorical_var_name in categorical_var_names:\n",
    "    categorical_var = pd.Categorical(\n",
    "        df.loc[:, categorical_var_name])\n",
    "    \n",
    "    # Just have one dummy variable if it's boolean\n",
    "    if len(categorical_var.categories) == 2:\n",
    "        drop_first = True\n",
    "    else:\n",
    "        drop_first = False\n",
    "\n",
    "    dummies = pd.get_dummies(\n",
    "        categorical_var, \n",
    "        prefix=categorical_var_name,\n",
    "        drop_first=drop_first)\n",
    "    \n",
    "    X = pd.concat([X, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the Y labels\n",
    "readmitted = pd.Categorical(df.readmitted)\n",
    "\n",
    "Y = np.copy(readmitted.codes)\n",
    "\n",
    "# Combine >30 and 0 and flip labels, so 1 (>30) and 2 (No) become -1, while 0 becomes 1\n",
    "Y[Y >= 1] = -1\n",
    "Y[Y == 0] = 1\n",
    "\n",
    "# Map to feature names\n",
    "feature_names = X.columns.values\n",
    "\n",
    "### Find indices of age features\n",
    "age_var = pd.Categorical(df.loc[:, 'age'])\n",
    "age_var_names = ['age_%s' % age_var_name for age_var_name in age_var.categories]    \n",
    "age_var_indices = []\n",
    "for age_var_name in age_var_names:\n",
    "    age_var_indices.append(np.where(X.columns.values == age_var_name)[0][0])\n",
    "age_var_indices = np.array(age_var_indices, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split into training and test sets. \n",
    "# For convenience, we balance the training set to have 10k positives and 10k negatives.\n",
    "\n",
    "np.random.seed(2)\n",
    "num_examples = len(Y)\n",
    "assert X.shape[0] == num_examples\n",
    "num_train_examples = 20000\n",
    "num_train_examples_per_class = int(num_train_examples / 2)\n",
    "num_test_examples = num_examples - num_train_examples\n",
    "assert num_test_examples > 0\n",
    "\n",
    "pos_idx = np.where(Y == 1)[0]\n",
    "neg_idx = np.where(Y == -1)[0]\n",
    "np.random.shuffle(pos_idx)\n",
    "np.random.shuffle(neg_idx)\n",
    "assert len(pos_idx) + len(neg_idx) == num_examples\n",
    "\n",
    "train_idx = np.concatenate((pos_idx[:num_train_examples_per_class], neg_idx[:num_train_examples_per_class]))\n",
    "test_idx = np.concatenate((pos_idx[num_train_examples_per_class:], neg_idx[num_train_examples_per_class:]))\n",
    "np.random.shuffle(train_idx)\n",
    "np.random.shuffle(test_idx)\n",
    "\n",
    "X_train = np.array(X.iloc[train_idx, :], dtype=np.float32)\n",
    "Y_train = Y[train_idx]\n",
    "\n",
    "X_test = np.array(X.iloc[test_idx, :], dtype=np.float32)\n",
    "Y_test = Y[test_idx]\n",
    "\n",
    "#train = DataSet(X_train, Y_train)\n",
    "#validation = None\n",
    "#test = DataSet(X_test, Y_test)\n",
    "#data_sets = base.Datasets(train=train, validation=validation, test=test)\n",
    "\n",
    "lr_train = DataSet(X_train, np.array((Y_train + 1) / 2, dtype=int))\n",
    "lr_validation = None\n",
    "lr_test = DataSet(X_test, np.array((Y_test + 1) / 2, dtype=int))\n",
    "lr_data_sets = base.Datasets(train=lr_train, validation=lr_validation, test=lr_test)\n",
    "\n",
    "test_children_idx = np.where(X_test[:, age_var_indices[0]] == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique((Y_test + 1) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model on the training set\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "weight_decay = 0.0001\n",
    "batch_size = 100\n",
    "initial_learning_rate = 0.001 \n",
    "keep_probs = None\n",
    "decay_epochs = [1000, 10000]\n",
    "max_lbfgs_iter = 1000\n",
    "use_bias = True\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "orig_model = BinaryLogisticRegressionWithLBFGS(\n",
    "    input_dim=input_dim,\n",
    "    weight_decay=weight_decay,\n",
    "    max_lbfgs_iter=max_lbfgs_iter,\n",
    "    num_classes=num_classes, \n",
    "    batch_size=batch_size,\n",
    "    data_sets=lr_data_sets,\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    keep_probs=keep_probs,\n",
    "    decay_epochs=decay_epochs,\n",
    "    mini_batch=False,\n",
    "    train_dir='output',\n",
    "    log_dir='log',\n",
    "    model_name='diabetes_logreg')\n",
    "\n",
    "orig_model.train()\n",
    "\n",
    "orig_model_preds = orig_model.sess.run(\n",
    "    orig_model.preds,\n",
    "    feed_dict=orig_model.all_test_feed_dict)\n",
    "print(orig_model_preds)\n",
    "orig_model_preds = orig_model_preds[test_children_idx, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove from the training set all but one young patients who didn't get readmitted \n",
    "mask_to_remove = (Y_train == -1) & (X_train[:, age_var_indices[0]] == 1) \n",
    "idx_to_remove = np.where(mask_to_remove)[0][:-1] # Keep 1 of them\n",
    "mask_to_keep = np.array([True] * len(mask_to_remove), dtype=bool)\n",
    "mask_to_keep[idx_to_remove] = False\n",
    "\n",
    "modified_X_train = np.copy(X_train)\n",
    "modified_Y_train = np.copy(Y_train)\n",
    "\n",
    "modified_X_train = modified_X_train[mask_to_keep, :]\n",
    "modified_Y_train = modified_Y_train[mask_to_keep]\n",
    "\n",
    "print('In original data, %s/%s children were readmitted.' % (\n",
    "        np.sum((Y_train == 1) & (X_train[:, age_var_indices[0]] == 1)),\n",
    "        np.sum((X_train[:, age_var_indices[0]] == 1))))\n",
    "print('In modified data, %s/%s children were readmitted.' % (\n",
    "        np.sum((modified_Y_train == 1) & (modified_X_train[:, age_var_indices[0]] == 1)),\n",
    "        np.sum((modified_X_train[:, age_var_indices[0]] == 1))))\n",
    "\n",
    "#modified_train = DataSet(modified_X_train, modified_Y_train)\n",
    "#validation = None\n",
    "#test = DataSet(X_test, Y_test)\n",
    "#modified_data_sets = base.Datasets(train=modified_train, validation=validation, test=test)\n",
    "\n",
    "\n",
    "lr_modified_train = DataSet(modified_X_train, np.array((modified_Y_train + 1) / 2, dtype=int))\n",
    "lr_modified_data_sets = base.Datasets(train=lr_modified_train, validation=lr_validation, test=lr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model on the modified training set\n",
    "tf.reset_default_graph()\n",
    "\n",
    "modified_model = BinaryLogisticRegressionWithLBFGS(\n",
    "    input_dim=input_dim,\n",
    "    weight_decay=weight_decay,\n",
    "    max_lbfgs_iter=max_lbfgs_iter,\n",
    "    num_classes=num_classes, \n",
    "    batch_size=batch_size,\n",
    "    data_sets=lr_modified_data_sets,\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    keep_probs=keep_probs,\n",
    "    decay_epochs=decay_epochs,\n",
    "    mini_batch=False,\n",
    "    train_dir='output',\n",
    "    log_dir='log',\n",
    "    model_name='diabetes_logreg')\n",
    "\n",
    "modified_model.train()\n",
    "\n",
    "modified_model_preds = modified_model.sess.run(\n",
    "    modified_model.preds,\n",
    "    feed_dict=modified_model.all_test_feed_dict)\n",
    "modified_model_preds = modified_model_preds[test_children_idx, 0]\n",
    "modified_theta = modified_model.sess.run(modified_model.params)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: look at coefficient values\n",
    "sns.set_style('white')\n",
    "plt.figure(figsize=(8, 10))\n",
    "idx = np.argsort(np.abs(modified_theta))[-20:]\n",
    "sns.barplot(np.abs(modified_theta[idx]), X.columns.values[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find children in the test set and see how predictions change on them\n",
    "true_labels = Y_test[test_children_idx]\n",
    "\n",
    "for i in range(len(test_children_idx)):\n",
    "    if (orig_model_preds[i] < 0.5) != (modified_model_preds[i] < 0.5):\n",
    "        print('*** ', end='')\n",
    "    print(\"index %s, label %s: %s vs. %s\" % (\n",
    "        test_children_idx[i], true_labels[i], \n",
    "        orig_model_preds[i], modified_model_preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one of those children and find the most influential examples on it\n",
    "test_idx = 1742\n",
    "x_test = X_test[test_idx, :]\n",
    "y_test = Y_test[test_idx]\n",
    "print(\"Test point features:\")\n",
    "print(x_test)\n",
    "print(y_test)\n",
    "print('Younger than 10? %s' % x_test[age_var_indices[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dissect get_influence_on_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../influence/\")\n",
    "from ihvp import get_inverse_hvp_cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_influence_on_test_loss(trained_model, test_indices, train_idx, \n",
    "    approx_type='cg', approx_params=None, force_refresh=True, test_description=None,\n",
    "    loss_type='normal_loss',\n",
    "    X=None, Y=None):\n",
    "    # If train_idx is None then use X and Y (phantom points)\n",
    "    # Need to make sure test_idx stays consistent between models\n",
    "    # because mini-batching permutes dataset order\n",
    "\n",
    "    if train_idx is None: \n",
    "        if (X is None) or (Y is None): raise ValueError('X and Y must be specified if using phantom points.')\n",
    "        if X.shape[0] != len(Y): raise ValueError('X and Y must have the same length.')\n",
    "    else:\n",
    "        if (X is not None) or (Y is not None): raise ValueError('X and Y cannot be specified if train_idx is specified.')\n",
    "    \n",
    "    test_grad_loss_no_reg_val = trained_model.get_test_grad_loss_no_reg_val(test_indices, loss_type=loss_type)\n",
    "\n",
    "    print('Norm of test gradient: %s' % np.linalg.norm(np.concatenate(test_grad_loss_no_reg_val)))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if test_description is None:\n",
    "        test_description = test_indices\n",
    "\n",
    "    #approx_filename = os.path.join(trained_model.train_dir, '%s-%s-%s-test-%s.npz' % (self.model_name, approx_type, loss_type, test_description))\n",
    "    #if os.path.exists(approx_filename) and force_refresh == False:\n",
    "    #    inverse_hvp = list(np.load(approx_filename)['inverse_hvp'])\n",
    "    #    print('Loaded inverse HVP from %s' % approx_filename)\n",
    "    #else:\n",
    "    inverse_hvp = get_inverse_hvp_cg(\n",
    "        modified_model,\n",
    "        test_grad_loss_no_reg_val# ,\n",
    "        # approx_type,\n",
    "        # approx_params\n",
    "    )\n",
    "    #    np.savez(approx_filename, inverse_hvp=inverse_hvp)\n",
    "    #    print('Saved inverse HVP to %s' % approx_filename)\n",
    "    np.save(\"inverse_hvp_tf.npy\", inverse_hvp)\n",
    "    duration = time.time() - start_time\n",
    "    print('Inverse HVP took %s sec' % duration)\n",
    "\n",
    "\n",
    "    print(\"Number of training examples\", trained_model.num_train_examples)\n",
    "    start_time = time.time()\n",
    "    if train_idx is None:\n",
    "        num_to_remove = len(Y)\n",
    "        predicted_loss_diffs = np.zeros([num_to_remove])            \n",
    "        for counter in np.arange(num_to_remove):\n",
    "            single_train_feed_dict = trained_model.fill_feed_dict_manual(X[counter, :], [Y[counter]])      \n",
    "            train_grad_loss_val = trained_model.sess.run(trained_model.grad_total_loss_op, feed_dict=single_train_feed_dict)\n",
    "            predicted_loss_diffs[counter] = np.dot(np.concatenate(inverse_hvp), np.concatenate(train_grad_loss_val)) / trained_model.num_train_examples            \n",
    "\n",
    "    else:            \n",
    "        num_to_remove = len(train_idx)\n",
    "        train_grad_loss_list = np.zeros([num_to_remove, 127])\n",
    "        predicted_loss_diffs = np.zeros([num_to_remove])\n",
    "        for counter, idx_to_remove in enumerate(train_idx):            \n",
    "            single_train_feed_dict = trained_model.fill_feed_dict_with_one_ex(trained_model.data_sets.train, idx_to_remove)      \n",
    "            train_grad_loss_val = trained_model.sess.run(trained_model.grad_total_loss_op, feed_dict=single_train_feed_dict)\n",
    "            predicted_loss_diffs[counter] = np.dot(np.concatenate(inverse_hvp), np.concatenate(train_grad_loss_val)) / trained_model.num_train_examples\n",
    "            # print(train_grad_loss_val[0])\n",
    "            train_grad_loss_list[counter, :] = train_grad_loss_val[0]\n",
    "    \n",
    "    np.save(\"train_grad_tf.npy\", train_grad_loss_list)\n",
    "    print(\"train_grad_loss_list\", train_grad_loss_list)\n",
    "    duration = time.time() - start_time\n",
    "    print('Multiplying by %s train examples took %s sec' % (num_to_remove, duration))\n",
    "\n",
    "    return predicted_loss_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified_model.sess.run(modified_model.v_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influences = get_influence_on_test_loss(modified_model,\n",
    "                                        test_indices=[1742]\n",
    "                                        ,train_idx=np.arange(len(modified_model.data_sets.train.labels))\n",
    "                                        ,force_refresh=False\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(modified_model.data_sets.train.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue with original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "helpful_points = np.argsort(influences)[-top_k:][::-1]\n",
    "unhelpful_points = np.argsort(influences)[:top_k]\n",
    "\n",
    "influences_to_plot = []\n",
    "ages_to_plot = []\n",
    "\n",
    "for points, message in [\n",
    "    (unhelpful_points, 'worse'), (helpful_points, 'better')]:\n",
    "    print(\"Top %s training points making the loss on the test point %s:\" % (top_k, message))\n",
    "    for counter, idx in enumerate(points):\n",
    "        print(\"#%5d, class=%s, age=%s, predicted_loss_diff=%.8f\" % (\n",
    "            idx,                 \n",
    "            modified_Y_train[idx], \n",
    "            modified_X_train[idx, age_var_indices],\n",
    "            influences[idx]))\n",
    "        \n",
    "        ages_to_plot.append(idx)\n",
    "        influences_to_plot.append(influences[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The children in the modified dataset are by far the most influential\n",
    "plt.figure(figsize=(15,6))\n",
    "sort_idx = np.argsort(influences_to_plot)\n",
    "ages_to_plot = np.array(ages_to_plot)\n",
    "sns.barplot(ages_to_plot, influences_to_plot, order=ages_to_plot[sort_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_of_influence_wrt_input(trained_model, train_indices, test_indices, \n",
    "    approx_type='cg', approx_params=None, force_refresh=True, verbose=True, test_description=None,\n",
    "    loss_type='normal_loss'):\n",
    "    \"\"\"\n",
    "    If the loss goes up when you remove a point, then it was a helpful point.\n",
    "    So positive influence = helpful.\n",
    "    If we move in the direction of the gradient, we make the influence even more positive, \n",
    "    so even more helpful.\n",
    "    Thus if we want to make the test point more wrong, we have to move in the opposite direction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate v_placeholder (gradient of loss at test point)\n",
    "    test_grad_loss_no_reg_val = trained_model.get_test_grad_loss_no_reg_val(test_indices, loss_type=loss_type)            \n",
    "\n",
    "    if verbose: print('Norm of test gradient: %s' % np.linalg.norm(np.concatenate(test_grad_loss_no_reg_val)))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if test_description is None:\n",
    "        test_description = test_indices\n",
    "\n",
    "    #approx_filename = os.path.join(trained_model.train_dir, '%s-%s-%s-test-%s.npz' % (trained_model.model_name, approx_type, loss_type, test_description))\n",
    "\n",
    "    #if os.path.exists(approx_filename) and force_refresh == False:\n",
    "    #    inverse_hvp = list(np.load(approx_filename)['inverse_hvp'])\n",
    "    #    if verbose: print('Loaded inverse HVP from %s' % approx_filename)\n",
    "    #else:            \n",
    "    inverse_hvp = trained_model.get_inverse_hvp(\n",
    "        test_grad_loss_no_reg_val,\n",
    "        approx_type,\n",
    "        approx_params,\n",
    "        verbose=verbose)\n",
    "    #np.savez(approx_filename, inverse_hvp=inverse_hvp)\n",
    "    #if verbose: print('Saved inverse HVP to %s' % approx_filename)            \n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    if verbose: print('Inverse HVP took %s sec' % duration)\n",
    "\n",
    "    grad_influence_wrt_input_val = None\n",
    "\n",
    "    for counter, train_idx in enumerate(train_indices):\n",
    "        # Put in the train example in the feed dict\n",
    "        grad_influence_feed_dict = trained_model.fill_feed_dict_with_one_ex(\n",
    "            trained_model.data_sets.train,  \n",
    "            train_idx)\n",
    "\n",
    "        trained_model.update_feed_dict_with_v_placeholder(grad_influence_feed_dict, inverse_hvp)\n",
    "        \n",
    "        #print(\"grad_influence_feed_dict\", trained_model.sess.run(grad_influence_feed_dict))\n",
    "        \n",
    "        # Run the grad op with the feed dict\n",
    "        current_grad_influence_wrt_input_val = trained_model.sess.run(trained_model.grad_influence_wrt_input_op, feed_dict=grad_influence_feed_dict)[0][0, :]            \n",
    "\n",
    "        if grad_influence_wrt_input_val is None:\n",
    "            grad_influence_wrt_input_val = np.zeros([len(train_indices), len(current_grad_influence_wrt_input_val)])\n",
    "\n",
    "        grad_influence_wrt_input_val[counter, :] = current_grad_influence_wrt_input_val\n",
    "\n",
    "    return grad_influence_wrt_input_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at which features are causing this influence\n",
    "grad_influences_wrt_input_val = get_grad_of_influence_wrt_input(\n",
    "    modified_model,\n",
    "    [19590, 13685, 9366, 11116], \n",
    "    [test_idx], \n",
    "    force_refresh=False,\n",
    "    test_description=None,\n",
    "    loss_type='normal_loss')    \n",
    "\n",
    "delta = grad_influences_wrt_input_val[0, :]\n",
    "plt.figure(figsize=(8, 6))\n",
    "idx_to_plot = np.array([0] * len(delta), dtype=bool)\n",
    "idx_to_plot[:10] = 1\n",
    "idx_to_plot[-10:] = 1\n",
    "sns.barplot(np.sort(delta)[idx_to_plot], feature_names[np.argsort(delta)[idx_to_plot]], orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_filename = 'output/diabetes_logreg-cg-normal_loss-test-[1742].npz'\n",
    "inverse_hvp = list(np.load(approx_filename)['inverse_hvp'])\n",
    "inverse_hvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_influences_wrt_input_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
